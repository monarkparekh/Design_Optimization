{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu8e+j+QdV09gBQH1UFy4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monarkparekh/MAE-598__Design-Optimization/blob/Assignment-2/MAE_598__Design_Optimization_HW2_Q2b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h1>MAE 598 Design Optimization: \n",
        "Assignment 2, Question 2b</b></h1>\n",
        "\n",
        "Name: Monark Parekh <br> \n",
        "ASU ID: 1222179426\n"
      ],
      "metadata": {
        "id": "_UWtZTkndDI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required libraries"
      ],
      "metadata": {
        "id": "q1Z-tpSDfFIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "MWyZs87wfE6K"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Function, Gradient and Hessian matrix"
      ],
      "metadata": {
        "id": "BD3_0hxOfoLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "tGnBitN1c446"
      },
      "outputs": [],
      "source": [
        "function = lambda x: (2-2*x[0]-3*x[1])**2 + (x[0])**2 + (x[1]-1)**2\n",
        "\n",
        "def gradient(x): \n",
        "  return np.array([(10*x[0]+12*x[1]-8), (12*x[0]+20*x[1]-14)])\n",
        "\n",
        "Hessian = np.array([[10,12],[12,20]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Line Search Algorithm</b>"
      ],
      "metadata": {
        "id": "5Du4s_EUnrsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linesearchalgorithm(x):\n",
        "    alpha = 1\n",
        "    t = 0.3\n",
        "    temp = -1 * gradient(x)\n",
        "    def pi(alpha, x):\n",
        "      return function(x) - alpha * t * np.matmul(np.transpose(gradient(x)), temp)\n",
        "    while pi(alpha, x) < function(x + alpha*temp):\n",
        "      alpha = 0.5 * alpha\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "r9nTN4h65ZAf"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linesearchalgorithm(x):\n",
        "  # Defining Parameters\n",
        "  alpha = 1\n",
        "  t = 0.3\n",
        "  beta = 0.5\n",
        "  # Defining the function to calculate phi(alpha)\n",
        "  def phi(alpha, x):\n",
        "    temp = -1 * gradient(x)\n",
        "    gTg = alpha * t * np.matmul(np.transpose(gradient(x)), temp)\n",
        "    return (function(x) - (gTg))\n",
        "  # Comparing phi(alpha) and f(x - alpha*gradient)\n",
        "  while phi(a, x) < function(x + (alpha*[-1*gradient(x)])):\n",
        "    alpha = beta*alpha\n",
        "  # Returning the Final value of alpha\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "gEXSPdIXfP6N"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Gradient Descent Algorithm</b>"
      ],
      "metadata": {
        "id": "H5moBRrLrQkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientdescentalgorithm(function,gradient,Hessian,x0,maximum_iteration):\n",
        "  epsilon = (10)**-6\n",
        "  X = []\n",
        "  gradient_normal = []\n",
        "  X.append(x0)\n",
        "  for k in range(0,maximum_iteration):\n",
        "      gradient_normal.append(np.linalg.norm(gradient(X[k])))\n",
        "\n",
        "      if gradient_normal[k] <= epsilon:\n",
        "        x1 = 1-(2*X[k][0]+3*X[k][1])\n",
        "        error_values = [abs(function(X[i])-function([-1/7,11/14])) for i in range(len(X))]\n",
        "        X = np.insert(X,0,x1,axis=1)\n",
        "        print(f'\\nThe Gradient Descent Algorithm has converged to a point in {k} iterations\\n')\n",
        "        print(f'Current Gradient Normal is {gradient_normal[k]}\\n[x1,x2,x3] = {X[k]}')\n",
        "        return X[k],gradient_normal,(k),error_values\n",
        "      \n",
        "      alpha = linesearchalgorithm(X[k])\n",
        "      new_X = X[k] - alpha*gradient(X[k])\n",
        "      X.append(new_X)\n",
        "\n",
        "      if k == (maximum_iteration - 1):\n",
        "        print(f\"Number of iterations has exceeded the maxinimum iterations, still not converged to the given threshold.\\nCurrent Gradient Normal is {gradient_normal[k]}\\nCurrent point is {X[k]}\")\n",
        "        return X[k],gradient_normal,(k)\n",
        "\n",
        " \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rKawFJsafkXd"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val,gradient_normal_val,(k),error_values_all  =  gradientdescentalgorithm(function,gradient,Hessian,np.array([0,0]),1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFDY_b_YrDJs",
        "outputId": "f13c0a49-72fa-425c-e0f6-4daeb0f780d6"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The algorithm has converged to a point in 100 iterations\n",
            "\n",
            "Current Gradient Normal is 9.516108713790478e-07\n",
            "[x1,x2,x3] = [-1.07142846 -0.142857    0.78571416]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Newton's Algorithm</b>"
      ],
      "metadata": {
        "id": "GCeFaopUGNSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newtonsalgorithm(function,gradient,Hessian,x0,maximum_iteration):\n",
        "  epsilon = (10)**-6\n",
        "  X = []\n",
        "  gradient_normal = []\n",
        "  X.append(x0)\n",
        "  for k in range(0,maximum_iteration):\n",
        "      gradient_normal.append(np.linalg.norm(gradient(X[k])))\n",
        "\n",
        "      if gradient_normal[k] <= epsilon:\n",
        "        x1 = 1-(2*X[k][0]+3*X[k][1])\n",
        "        error_values = [abs(function(X[i])-function([-1/7,11/14])) for i in range(len(X))]\n",
        "        X = np.insert(X,0,x1,axis=1)\n",
        "        print(f'\\nThe Newtons Algorithm has converged to a point in {k} iterations\\n')\n",
        "        print(f'Current Gradient Normal is {gradient_normal[k]}\\n[x1,x2,x3] = {X[k]}')\n",
        "        return X[k],gradient_normal,(k),error_values\n",
        "      \n",
        "      new_X = X[k] - np.matmul(np.linalg.inv(Hessian), gradient(X[k]))\n",
        "      X.append(new_X)\n",
        "\n",
        "      if k == (maximum_iteration - 1):\n",
        "        print(f\"Number of iterations has exceeded the maxinimum iterations, still not converged to the given threshold.\\nCurrent Gradient Normal is {gradient_normal[k]}\\nCurrent point is {X[k]}\")\n",
        "        return X[k],gradient_normal,(k)\n",
        "\n",
        " \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Roav-3KXGMaR"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val,gradient_normal_val,(k),error_values_all  =  newtonsalgorithm(function,gradient,Hessian,np.array([0,0]),1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq5H-_8A91eM",
        "outputId": "acf93804-ea8a-41dd-db53-fb0095ee397f"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Newtons Algorithm has converged to a point in 1 iterations\n",
            "\n",
            "Current Gradient Normal is 3.972054645195637e-15\n",
            "[x1,x2,x3] = [-1.07142857 -0.14285714  0.78571429]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XWx5Phm92jr"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owp8G2fa_2yj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}